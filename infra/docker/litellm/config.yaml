# DataMind — LiteLLM Proxy Configuration
# Unified LLM routing with Langfuse observability + SLM-based routing

model_list:

  # ---- Anthropic Claude (Primary — reasoning + reports) ----
  - model_name: claude-sonnet-4-6
    litellm_params:
      model: anthropic/claude-sonnet-4-6
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 16384
      stream: true

  - model_name: claude-opus-4-6
    litellm_params:
      model: anthropic/claude-opus-4-6
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 32768

  # ---- OpenAI GPT-4o (Fallback) ----
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 16384

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 8192

  # ---- Google Gemini (Multi-modal fallback) ----
  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GOOGLE_API_KEY
      max_tokens: 32768

  # ---- Ollama Local SLMs (< 100ms, no cloud) ----
  - model_name: phi3.5
    litellm_params:
      model: ollama/phi3.5
      api_base: http://ollama:11434
      stream: true

  - model_name: gemma2:2b
    litellm_params:
      model: ollama/gemma2:2b
      api_base: http://ollama:11434
      stream: true

  # ---- Ollama Local LLMs ----
  - model_name: llama3.3:70b
    litellm_params:
      model: ollama/llama3.3:70b
      api_base: http://ollama:11434
      stream: true
      timeout: 300

  - model_name: codestral:22b
    litellm_params:
      model: ollama/codestral:22b
      api_base: http://ollama:11434
      stream: true
      timeout: 120

  - model_name: deepseek-r1:32b
    litellm_params:
      model: ollama/deepseek-r1:32b
      api_base: http://ollama:11434
      stream: true
      timeout: 600   # RLM — longer thinking budget

  # ---- Router: intent-based auto routing ----
  - model_name: datamind-router
    litellm_params:
      model: anthropic/claude-sonnet-4-6
      api_key: os.environ/ANTHROPIC_API_KEY

router_settings:
  routing_strategy: latency-based-routing
  num_retries: 3
  timeout: 120
  retry_after: 2
  allowed_fails: 2
  cooldown_time: 60

  # Token budget enforcement per agent role (GDPR + cost control)
  max_tokens_per_model:
    phi3.5: 2048
    gemma2:2b: 2048
    codestral:22b: 8192
    claude-sonnet-4-6: 16384
    deepseek-r1:32b: 32768

litellm_settings:
  # Langfuse observability callback — ALL calls traced
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

  langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
  langfuse_secret_key: os.environ/LANGFUSE_SECRET_KEY
  langfuse_host: os.environ/LANGFUSE_HOST

  # Prompt caching — system prompts cached across requests
  cache: true
  cache_params:
    type: redis
    host: redis
    port: 6379
    password: os.environ/REDIS_PASSWORD
    ttl: 600  # 10 min cache for system prompts

  # Drop unsupported params across providers
  drop_params: true
  set_verbose: false

  # Cost tracking — fed into Stripe usage metering
  track_cost_callback: true

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true

  # Rate limiting (Kong handles external, LiteLLM handles per-model)
  max_parallel_requests: 100
  request_timeout: 120

  # OTEL tracing
  otel_endpoint: http://otel-collector:4317
