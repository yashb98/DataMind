name: datamind

# ============================================================
# DataMind — Local Development Docker Compose
# Day 2: +SLM Router, Embedding Service, Kong API Gateway
# Day 3: +Auth Service, updated Kong with JWT+ABAC
#
# Usage:
#   docker compose --profile dev up -d          # all dev services
#   docker compose --profile inference up -d    # + vLLM + Ollama
#   docker compose --profile observability up   # + Grafana stack
#   docker compose ps                           # health check
# ============================================================

x-common-env: &common-env
  TZ: UTC
  DATAMIND_ENV: development

x-restart-policy: &restart-policy
  restart: unless-stopped

x-logging: &logging
  logging:
    driver: "json-file"
    options:
      max-size: "50m"
      max-file: "3"

services:

  # ============================================================
  # 1. PostgreSQL 16 — Core relational DB + pgvector + TimescaleDB
  # ============================================================
  postgres:
    image: timescale/timescaledb-ha:pg16
    container_name: datamind-postgres
    <<: [*restart-policy, *logging]
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-datamind}
      POSTGRES_USER: ${POSTGRES_USER:-datamind}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --locale=en_US.UTF-8"
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./infra/docker/postgres/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-datamind} -d ${POSTGRES_DB:-datamind}"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dev", "core"]

  # ============================================================
  # 2. ClickHouse — OLAP analytics + Langfuse backend
  # ============================================================
  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: datamind-clickhouse
    <<: [*restart-policy, *logging]
    environment:
      CLICKHOUSE_DB: ${CLICKHOUSE_DB:-analytics}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-default}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-changeme}
    ports:
      - "8123:8123"   # HTTP interface
      - "9000:9000"   # Native TCP
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - ./infra/docker/clickhouse/config.xml:/etc/clickhouse-server/config.d/custom.xml:ro
      - ./infra/docker/clickhouse/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dev", "core"]

  # ============================================================
  # 3. Qdrant — Vector store for RAG and agent memory
  # ============================================================
  qdrant:
    image: qdrant/qdrant:v1.12.0
    container_name: datamind-qdrant
    <<: [*restart-policy, *logging]
    environment:
      QDRANT__SERVICE__API_KEY: ${QDRANT_API_KEY:-}
      QDRANT__LOG_LEVEL: INFO
    ports:
      - "6333:6333"   # HTTP
      - "6334:6334"   # gRPC
    volumes:
      - qdrant-data:/qdrant/storage
      - ./infra/docker/qdrant/config.yaml:/qdrant/config/production.yaml:ro
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:6333/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dev", "core"]

  # ============================================================
  # 4. Redis 7 — Cache, session state, rate limiting
  # ============================================================
  redis:
    image: redis:7.4-alpine
    container_name: datamind-redis
    <<: [*restart-policy, *logging]
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD:-changeme}
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-changeme}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dev", "core"]

  # ============================================================
  # 5. MongoDB — Episodic agent memory, unstructured logs
  # ============================================================
  mongodb:
    image: mongo:7.0
    container_name: datamind-mongodb
    <<: [*restart-policy, *logging]
    environment:
      MONGO_INITDB_ROOT_USERNAME: datamind
      MONGO_INITDB_ROOT_PASSWORD: ${MONGODB_PASSWORD:-changeme}
      MONGO_INITDB_DATABASE: datamind
    ports:
      - "27017:27017"
    volumes:
      - mongodb-data:/data/db
      - ./infra/docker/mongodb/init.js:/docker-entrypoint-initdb.d/init.js:ro
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dev", "core"]

  # ============================================================
  # 6. Neo4j — GraphRAG, data lineage graph
  # ============================================================
  neo4j:
    image: neo4j:5.24-enterprise
    container_name: datamind-neo4j
    <<: [*restart-policy, *logging]
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD:-changeme}
      NEO4J_PLUGINS: '["apoc", "graph-data-science"]'
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      NEO4J_server_memory_heap_initial__size: 512m
      NEO4J_server_memory_heap_max__size: 2G
    ports:
      - "7474:7474"   # Browser UI
      - "7687:7687"   # Bolt
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
    profiles: ["dev", "core"]

  # ============================================================
  # 7. MinIO — Object storage (S3-compatible) for Iceberg + artifacts
  # ============================================================
  minio:
    image: minio/minio:RELEASE.2024-11-07T00-52-20Z
    container_name: datamind-minio
    <<: [*restart-policy, *logging]
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minioadmin}
      MINIO_DEFAULT_BUCKETS: datamind-iceberg,datamind-artifacts,datamind-reports,datamind-models
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Console UI
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dev", "core"]

  # MinIO bucket initialisation
  minio-init:
    image: minio/mc:RELEASE.2024-11-17T19-35-25Z
    container_name: datamind-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set local http://minio:9000 ${MINIO_ACCESS_KEY:-minioadmin} ${MINIO_SECRET_KEY:-minioadmin};
      mc mb --ignore-existing local/datamind-iceberg;
      mc mb --ignore-existing local/datamind-artifacts;
      mc mb --ignore-existing local/datamind-reports;
      mc mb --ignore-existing local/datamind-models;
      echo 'MinIO buckets created';
      "
    profiles: ["dev", "core"]

  # ============================================================
  # 8. Apache Kafka + Zookeeper + Schema Registry
  # ============================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.7.1
    container_name: datamind-zookeeper
    <<: [*restart-policy, *logging]
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dev", "core"]

  kafka:
    image: confluentinc/cp-kafka:7.7.1
    container_name: datamind-kafka
    <<: [*restart-policy, *logging]
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
    ports:
      - "9092:9092"
    volumes:
      - kafka-data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 15s
      timeout: 10s
      retries: 10
    profiles: ["dev", "core"]

  schema-registry:
    image: confluentinc/cp-schema-registry:7.7.1
    container_name: datamind-schema-registry
    <<: [*restart-policy, *logging]
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 15s
      timeout: 10s
      retries: 10
    profiles: ["dev", "core"]

  # Kafka topic initialisation
  kafka-init:
    image: confluentinc/cp-kafka:7.7.1
    container_name: datamind-kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: >
      /bin/bash -c "
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --replication-factor 1 --partitions 6 --topic raw.ingestion;
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --replication-factor 1 --partitions 6 --topic agent.events;
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --replication-factor 1 --partitions 3 --topic worker.tasks;
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --replication-factor 1 --partitions 3 --topic dq.alerts;
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --replication-factor 1 --partitions 3 --topic breach.detection;
      echo 'Kafka topics created';
      "
    profiles: ["dev", "core"]

  # ============================================================
  # 9. Project Nessie — Iceberg REST Catalog
  # ============================================================
  nessie:
    image: ghcr.io/projectnessie/nessie:0.99.0
    container_name: datamind-nessie
    <<: [*restart-policy, *logging]
    environment:
      NESSIE_VERSION_STORE_TYPE: JDBC
      QUARKUS_DATASOURCE_JDBC_URL: jdbc:postgresql://postgres:5432/datamind
      QUARKUS_DATASOURCE_USERNAME: ${POSTGRES_USER:-datamind}
      QUARKUS_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
    ports:
      - "19120:19120"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/api/v1/config"]
      interval: 15s
      timeout: 10s
      retries: 10
    profiles: ["dev", "core"]

  # ============================================================
  # 10. LiteLLM Proxy — Unified LLM routing + cost tracking
  # ============================================================
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: datamind-litellm
    <<: [*restart-policy, *logging]
    command: ["--config", "/app/config.yaml", "--port", "4000", "--detailed_debug"]
    environment:
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY:-}
      LANGFUSE_HOST: ${LANGFUSE_HOST:-http://langfuse-web:3000}
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-}
      REDIS_URL: redis://:${REDIS_PASSWORD:-changeme}@redis:6379
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-litellm-dev}
      DATABASE_URL: ${DATABASE_URL:-postgresql://datamind:changeme@postgres:5432/datamind}
      STORE_MODEL_IN_DB: "True"
    ports:
      - "4000:4000"
    volumes:
      - ./infra/docker/litellm/config.yaml:/app/config.yaml:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health/liveliness"]
      interval: 15s
      timeout: 10s
      retries: 10
    profiles: ["dev", "core"]

  # ============================================================
  # 11. Langfuse — LLM Observability (self-hosted)
  # ============================================================
  langfuse-web:
    image: langfuse/langfuse:3
    container_name: datamind-langfuse
    <<: [*restart-policy, *logging]
    depends_on:
      postgres:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "3001:3000"
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-datamind}:${POSTGRES_PASSWORD:-changeme}@postgres:5432/langfuse
      DIRECT_URL: postgresql://${POSTGRES_USER:-datamind}:${POSTGRES_PASSWORD:-changeme}@postgres:5432/langfuse
      CLICKHOUSE_MIGRATION_URL: clickhouse://clickhouse:9000
      CLICKHOUSE_URL: http://clickhouse:8123
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-default}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-changeme}
      REDIS_CONNECTION_STRING: redis://:${REDIS_PASSWORD:-changeme}@redis:6379
      NEXTAUTH_URL: http://localhost:3001
      NEXTAUTH_SECRET: ${LANGFUSE_SALT:-change-me-in-production}
      SALT: ${LANGFUSE_SALT:-change-me-in-production}
      ENCRYPTION_KEY: ${LANGFUSE_ENCRYPTION_KEY:-0000000000000000000000000000000000000000000000000000000000000000}
      LANGFUSE_INIT_ORG_ID: datamind
      LANGFUSE_INIT_ORG_NAME: DataMind
      LANGFUSE_INIT_PROJECT_ID: datamind-platform
      LANGFUSE_INIT_PROJECT_NAME: DataMind Platform
      LANGFUSE_INIT_PROJECT_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-lf-pk-dev}
      LANGFUSE_INIT_PROJECT_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-lf-sk-dev}
      LANGFUSE_INIT_USER_EMAIL: admin@datamind.ai
      LANGFUSE_INIT_USER_NAME: Admin
      LANGFUSE_INIT_USER_PASSWORD: changeme
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/public/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
    profiles: ["dev", "core"]

  # ============================================================
  # 12. Ollama — Local SLM inference (Phi-3.5-mini, Gemma-2-2B)
  # ============================================================
  ollama:
    image: ollama/ollama:0.5.1
    container_name: datamind-ollama
    <<: [*restart-policy, *logging]
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles: ["dev", "inference"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Pull SLM models on startup
  ollama-pull:
    image: ollama/ollama:0.5.1
    container_name: datamind-ollama-pull
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      ollama pull phi3.5 &&
      ollama pull gemma2:2b &&
      echo 'SLM models ready'
      "
    environment:
      OLLAMA_HOST: http://ollama:11434
    profiles: ["dev", "inference"]

  # ============================================================
  # 13. Microsoft Presidio — PII detection + anonymisation
  # ============================================================
  presidio-analyzer:
    image: mcr.microsoft.com/presidio-analyzer:latest
    container_name: datamind-presidio-analyzer
    <<: [*restart-policy, *logging]
    ports:
      - "5001:3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 15s
      timeout: 10s
      retries: 5
    profiles: ["dev", "gdpr"]

  presidio-anonymizer:
    image: mcr.microsoft.com/presidio-anonymizer:latest
    container_name: datamind-presidio-anonymizer
    <<: [*restart-policy, *logging]
    ports:
      - "5002:3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 15s
      timeout: 10s
      retries: 5
    profiles: ["dev", "gdpr"]

  # ============================================================
  # Day 2 — SLM Router (intent classification + tier routing)
  # ============================================================
  slm-router:
    build:
      context: ./services/slm-router
      dockerfile: src/slm_router/Dockerfile
    container_name: datamind-slm-router
    <<: [*restart-policy, *logging]
    environment:
      OLLAMA_URL: http://ollama:11434
      REDIS_URL: redis://:${REDIS_PASSWORD:-changeme}@redis:6379
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-lf-pk-dev}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-lf-sk-dev}
      LANGFUSE_HOST: http://langfuse-web:3000
      OTEL_ENDPOINT: http://otel-collector:4317
    ports:
      - "8020:8020"
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8020/health/liveness').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles: ["dev", "core"]

  # ============================================================
  # Day 2 — Embedding Service (BAAI/bge-m3 + Qdrant init)
  # ============================================================
  embedding:
    build:
      context: ./services/embedding
    container_name: datamind-embedding
    <<: [*restart-policy, *logging]
    environment:
      QDRANT_URL: http://qdrant:6333
      QDRANT_API_KEY: ${QDRANT_API_KEY:-}
      OTEL_ENDPOINT: http://otel-collector:4317
      # Model cache dir — mount volume for fast restarts
      HF_HOME: /app/.cache/huggingface
    ports:
      - "8030:8030"
    volumes:
      - embedding-model-cache:/app/.cache/huggingface
    depends_on:
      qdrant:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8030/health/liveness').raise_for_status()"]
      interval: 60s
      timeout: 30s
      start_period: 120s   # Model download can take time on first run
      retries: 5
    profiles: ["dev", "core"]

  # ============================================================
  # Day 3 — Kong API Gateway (declarative config)
  # ============================================================
  kong-migrations:
    image: kong:3.8
    container_name: datamind-kong-migrations
    command: kong migrations bootstrap
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: postgres
      KONG_PG_PORT: 5432
      KONG_PG_DATABASE: kong
      KONG_PG_USER: ${POSTGRES_USER:-datamind}
      KONG_PG_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
    depends_on:
      postgres:
        condition: service_healthy
    profiles: ["dev", "core"]

  kong:
    image: kong:3.8
    container_name: datamind-kong
    <<: [*restart-policy, *logging]
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: postgres
      KONG_PG_PORT: 5432
      KONG_PG_DATABASE: kong
      KONG_PG_USER: ${POSTGRES_USER:-datamind}
      KONG_PG_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_PROXY_LISTEN: 0.0.0.0:8000
      KONG_PLUGINS: bundled,jwt,rate-limiting,cors,request-transformer,response-transformer,prometheus
      KONG_DECLARATIVE_CONFIG: /etc/kong/kong.yml
    ports:
      - "8000:8000"   # Proxy (external traffic)
      - "8001:8001"   # Admin API (internal only in prod)
      - "8443:8443"   # HTTPS proxy
    volumes:
      - ./infra/docker/kong/kong.yml:/etc/kong/kong.yml:ro
    depends_on:
      kong-migrations:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 15s
      timeout: 10s
      retries: 10
    profiles: ["dev", "core"]

  # ============================================================
  # Day 3 — Auth Service (JWT + multi-tenancy)
  # ============================================================
  auth:
    build:
      context: ./services/auth
    container_name: datamind-auth
    <<: [*restart-policy, *logging]
    environment:
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-datamind}:${POSTGRES_PASSWORD:-changeme}@postgres:5432/datamind
      REDIS_URL: redis://:${REDIS_PASSWORD:-changeme}@redis:6379
      JWT_SECRET_KEY: ${DATAMIND_SECRET_KEY:-change-me-in-production}
      VAULT_URL: ${VAULT_URL:-http://vault:8200}
      VAULT_TOKEN: ${VAULT_TOKEN:-root-token-dev-only}
      OTEL_ENDPOINT: http://otel-collector:4317
    ports:
      - "8010:8010"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8010/health/liveness').raise_for_status()"]
      interval: 20s
      timeout: 10s
      retries: 5
    profiles: ["dev", "core"]

  # ============================================================
  # 14. Observability Stack (Grafana + Prometheus + Loki + Jaeger)
  # ============================================================
  prometheus:
    image: prom/prometheus:v3.0.0
    container_name: datamind-prometheus
    <<: [*restart-policy, *logging]
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--web.enable-lifecycle"
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
      - ./infra/docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./infra/docker/prometheus/alerts:/etc/prometheus/alerts:ro
    profiles: ["dev", "observability"]

  grafana:
    image: grafana/grafana:11.3.0
    container_name: datamind-grafana
    <<: [*restart-policy, *logging]
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-changeme}
      GF_INSTALL_PLUGINS: grafana-clickhouse-datasource
    ports:
      - "3002:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./infra/docker/grafana/provisioning:/etc/grafana/provisioning:ro
    depends_on:
      - prometheus
    profiles: ["dev", "observability"]

  loki:
    image: grafana/loki:3.3.0
    container_name: datamind-loki
    <<: [*restart-policy, *logging]
    command: -config.file=/etc/loki/local-config.yaml
    ports:
      - "3100:3100"
    volumes:
      - loki-data:/loki
      - ./infra/docker/loki/local-config.yaml:/etc/loki/local-config.yaml:ro
    profiles: ["dev", "observability"]

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.114.0
    container_name: datamind-otel
    <<: [*restart-policy, *logging]
    command: ["--config=/etc/otel/config.yaml"]
    volumes:
      - ./infra/docker/otel/config.yaml:/etc/otel/config.yaml:ro
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "55679:55679" # zpages
    depends_on:
      - prometheus
      - loki
    profiles: ["dev", "observability"]

  jaeger:
    image: jaegertracing/all-in-one:1.63
    container_name: datamind-jaeger
    <<: [*restart-policy, *logging]
    environment:
      COLLECTOR_OTLP_ENABLED: "true"
    ports:
      - "16686:16686" # Jaeger UI
      - "14250:14250" # gRPC
    profiles: ["dev", "observability"]

# ============================================================
# Named Volumes
# ============================================================
volumes:
  postgres-data:
  clickhouse-data:
  qdrant-data:
  redis-data:
  mongodb-data:
  neo4j-data:
  neo4j-logs:
  minio-data:
  kafka-data:
  zookeeper-data:
  zookeeper-logs:
  ollama-models:
  embedding-model-cache:
  prometheus-data:
  grafana-data:
  loki-data:

# ============================================================
# Networks
# ============================================================
networks:
  default:
    name: datamind-network
    driver: bridge
